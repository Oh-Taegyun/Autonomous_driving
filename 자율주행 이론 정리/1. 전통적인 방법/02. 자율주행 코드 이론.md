### 1. introduction
---
색상 분리, 에지 감지, 선 분할 감지, 조향각 계산, 조향 안정화 등 알고리즘에 필요한 것들이 있다. 

또한, 파란색의 상한선과 하한선, Hough 변환을 통한 선분 감지의 여러 매개변수, 안정화 시 최대 조향 편차 등 많은 매개변수를 수동으로 조정해야 했다. 

이 모든 매개변수를 제대로 조정하지 않으면 차량이 원활하게 주행하지 못했다. 게다가 새로운 도로 조건이 생길 때마다 새로운 감지 알고리즘을 생각해 내어 차량에 프로그래밍해야 했고, 이는 매우 시간 소모적이며 유지 관리하기 어려웠다.

다행히 Nvidia 연구진들은 훌륭한 논문을 통해 실제 크기의 차량에 어떻게 운전하는지 "보여줌"으로써 그 차량이 스스로 운전을 배우는 것을 시연했다. 


### 2. Nvidia 모델  
---
높은 수준에서 Nvidia 모델의 입력은 차량에 장착된 DashCam에서 촬영한 비디오 이미지이고, 출력은 차량의 조향각이다. 이 모델은 비디오 이미지를 사용해 정보를 추출하고, 차량의 조향각을 예측하려고 한다. 

이는 비디오 이미지(특징)와 조향각(레이블)을 사용해 학습하는 지도 학습 프로그램으로 알려져 있다. 조향각은 수치 값이기 때문에, 이는 개나 고양이, 또는 이미지 속의 꽃 종류를 예측하는 분류 문제와 달리 회귀 문제이다.

Nvidia 모델의 핵심에는 합성곱 신경망(CNN, 케이블 네트워크가 아닙니다)이 있다. 


![[Pasted image 20241201164502.png]]

이 모델이 예측한 조향각은 주어진 비디오 이미지에서 원하는 조향각과 비교되며, 그 오차는 역전파를 통해 CNN 학습 과정으로 되돌아간다. 위 그림에서 보듯이, 이 과정은 오차(손실 또는 평균 제곱 오차)가 충분히 낮아질 때까지 반복된다. 이는 모델이 적절히 조향하는 법을 학습했음을 의미한다. 실제로 이는 매우 전형적인 이미지 인식 학습 과정이지만, 예측된 출력이 객체의 유형(분류)이 아닌 수치 값(회귀)이라는 점이 다르다.


### 3.DeepPiCar에 Nvidia 모델 적용하기  
---
크기를 제외하면, DeepPiCar는 Nvidia가 사용하는 차량과 매우 유사하다. DeepPiCar에도 대시캠이 있고, 조향각을 지정하여 제어할 수 있다. Nvidia는 다양한 주와 여러 대의 차량으로 70시간 동안 고속도로를 주행하면서 데이터를 수집했다. 

따라서 우리도 DeepPiCar의 비디오 영상을 수집하고 각 비디오 이미지에 대한 올바른 조향각을 기록해야 한다.


### 4. 데이터 수집  
---
학습 데이터를 수집하는 방법은 여러 가지가 있다.

첫 번째 방법은 원격 제어 프로그램을 작성하여 PiCar를 원격으로 조종하면서 각 프레임에서 비디오 프레임과 차량의 조향각을 저장하는 것이다. 이는 실제 사람의 운전 행동을 모사하기 때문에 아마도 가장 좋은 방법일 것이다. 하지만 원격 제어 프로그램을 작성해야 한다는 점에서 복잡하다. 

더 쉬운 방법은 우리가 Part 4에서 구축한 OpenCV 기반 차선 추적기를 활용하는 것이다. 해당 구현은 비교적 잘 작동하기 때문에 이를 우리의 "모델" 운전자로 사용할 수 있다. 기계가 다른 기계로부터 학습하는 것

우리가 해야 할 일은 OpenCV 구현을 트랙에서 몇 번 실행하고, 비디오 파일과 해당하는 조향각을 저장하는 것이다. 이후 Nvidia 모델을 학습시키기 위해 이를 사용할 수 있다. Part 4의 deep_pi_car.py에서는 차량을 실행할 때마다 자동으로 비디오 파일(AVI 파일)을 저장한다.

아래 코드는 비디오 파일을 사용해 개별 비디오 프레임을 저장하여 학습에 사용하도록 하는 코드이다. 간단히 하기 위해, 이미지 파일 이름에 조향각을 포함시켜 이미지 이름과 조향각 간의 매핑 파일을 따로 관리하지 않아도 되도록 했다.

``` python
import cv2
import sys
from hand_coded_lane_follower import HandCodedLaneFollower

# 비디오 파일에서 이미지와 조향각을 저장하는 함수
def save_image_and_steering_angle(video_file):
    lane_follower = HandCodedLaneFollower()  # HandCodedLaneFollower 객체 생성 (차선 인식을 위해 사용)
    cap = cv2.VideoCapture(video_file + '.avi')  # 지정된 비디오 파일(.avi) 열기

    try:
        i = 0
        while cap.isOpened():  # 비디오 캡처가 열려 있는 동안 반복
            _, frame = cap.read()  # 프레임 읽기
            if frame is None:  # 프레임이 없는 경우 종료
                break
            
            # 차선을 따라 주행하며 조향각을 계산
            lane_follower.follow_lane(frame)  # 현재 프레임을 이용해 차선 추적

            # 이미지 파일을 저장 (파일명에 인덱스와 현재 조향각 포함)
            cv2.imwrite("%s_%03d_%03d.png" % (video_file, i, lane_follower.curr_steering_angle), frame)
            i += 1

            # 'q' 키를 누르면 반복문 종료
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    finally:
        cap.release()  # 비디오 캡처 자원 해제
        cv2.destroyAllWindows()  # 모든 OpenCV 창 닫기

# 메인 함수
if __name__ == '__main__':
    # 커맨드라인 인자로 비디오 파일 이름을 받아와서 처리
    save_image_and_steering_angle(sys.argv[1])

```

이걸 돌리면 아래와 같이 나온다.

``` cmd
pi@raspberrypi:~/DeepPiCar/driver/code $ **ls ~/DeepPiCar/models/lane_navigation/data/images |more**  
video01_000_085.png  
video01_001_080.png  
video01_002_077.png  
video01_003_075.png  
video01_004_072.png  
video01_005_073.png  
video01_006_069.png
```

`.png` 접미사 앞의 세 숫자는 조향각을 나타낸다. 아래에서 볼 수 있듯이, 각도가 모두 90도보다 작아 차량이 왼쪽으로 회전하고 있었음을 알 수 있으며, 이는 위의 대시캠 영상을 확인해도 알 수 있다.


### 5. 학습/딥러닝
----
이제 특징(비디오 이미지)과 레이블(조향각)이 준비되었으니, 딥러닝을 시작할 시간이다. 

딥러닝 모델을 학습시키기 위해서는 Raspberry Pi의 CPU로는 부족하며, GPU의 힘이 필요하다. (그래서 라즈베리 파이로 데이터를 수집만하고 따로 수집된 데이터로 돌리는것이다. )

``` python
# 파이썬 표준 라이브러리 임포트
import os
import random
import fnmatch
import datetime
import pickle

# 데이터 처리 라이브러리 임포트
import numpy as np
np.set_printoptions(formatter={'float_kind': lambda x: "%.4f" % x})

import pandas as pd
pd.set_option('display.width', 300)
pd.set_option('display.float_format', '{:,.4f}'.format)
pd.set_option('display.max_colwidth', 200)

# PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, random_split
print(f'torch.__version__: {torch.__version__}')

# 사이킷런 라이브러리 임포트
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

# 이미지 처리 관련 라이브러리 임포트
import cv2
from imgaug import augmenters as img_aug
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image

# 이미지 로드 및 데이터셋 준비
!cd /content
!git clone https://github.com/dctian/DeepPiCar

!ls

data_dir = '/content/DeepPiCar/models/lane_navigation/data/images'
file_list = os.listdir(data_dir)
image_paths = []
steering_angles = []
pattern = "*.png"
for filename in file_list:
    if fnmatch.fnmatch(filename, pattern):
        image_paths.append(os.path.join(data_dir, filename))
        angle = int(filename[-7:-4])  # 092 부분은 video01_143_092.png에서 각도를 나타냄. 90은 직진을 의미
        steering_angles.append(angle)

# 신경망 모델 정의
class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        # Conv2D 레이어 정의
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)
        
        # MaxPool 레이어 정의
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        
        # Dropout 레이어 정의
        self.dropout = nn.Dropout(0.5)
        
        # Fully connected 레이어 정의
        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # 입력 크기는 Conv 레이어와 Pooling의 결과에 따라 조정해야 함
        self.fc2 = nn.Linear(512, 10)  # 출력 클래스 수 (예: 10 클래스 분류 문제)

    def forward(self, x):
        # Convolution, Activation, and Pooling Layers
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        
        # Flatten
        x = x.view(-1, 128 * 4 * 4)  # Flattening the tensor for the fully connected layer
        
        # Fully Connected Layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# 모델 초기화
model = CNNModel()

# 손실 함수 및 옵티마이저 정의
criterion = nn.CrossEntropyLoss()  # 분류 문제를 위한 손실 함수
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 모델 정보 출력
print(model)

# 데이터셋 로딩 예시 (추가적인 데이터 로딩 및 전처리 과정 필요)
# X_train, X_test, y_train, y_test = train_test_split(...)  # 데이터셋 분할
# train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))
# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# 모델 학습 루프 (예시)
# for epoch in range(num_epochs):
#     for inputs, labels in train_loader:
#         # Forward pass
#         outputs = model(inputs)
#         loss = criterion(outputs, labels)
#         
#         # Backward pass and optimization
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()

```


### 6. 학습 데이터 불러오기
---
그 다음으로는 학습 데이터를 불러와야 한다. 이번 글에서는 생성된 이미지 파일의 샘플 세트를 GitHub에 업로드했으니, 독자들이 이를 클론하여 따라올 수 있다. 이미지 파일 이름은 `videoXX_FFF_SSS.png` 형식으로 되어 있으며, 여기서 `videoXX`는 비디오 이름, `FFF`는 비디오의 프레임 번호, `SSS`는 조향각(도 단위)을 나타낸다. 

결과적으로 학습 데이터는 `image_paths`와 `steering_angles` 변수에 읽혀진다. 예를 들어, `video01_054_110.png`는 이 이미지가 `video01.avi` 비디오 파일에서 가져온 54번째 프레임이며, 조향각이 110도(오른쪽 회전)임을 의미한다.



1. 훈련/테스트 세트로 분할하기
	훈련 데이터를 sklearn의 `train_test_split` 메소드를 사용하여 80:20 비율로 훈련/검증 세트로 나눌 것입니다.

2. 이미지 증강
	샘플 학습 데이터 세트에는 약 200개의 이미지만 있습니다. 분명히 이것만으로는 딥러닝 모델을 학습시키기에 충분하지 않습니다. 그러나 우리는 간단한 기법인 이미지 증강(Image Augmentation)을 사용할 수 있습니다. 일반적인 증강 작업으로는 확대, 이동, 노출 값 변경, 블러 처리, 이미지 뒤집기 등이 있습니다. 원본 이미지에 이 다섯 가지 작업 중 임의로 하나 또는 모두를 적용함으로써 원본 200개의 이미지로부터 훨씬 더 많은 학습 데이터를 생성할 수 있으며, 이를 통해 최종 학습된 모델이 훨씬 더 견고해집니다. 아래에서는 확대와 뒤집기만을 설명하겠습니다. 다른 작업들도 유사하며, GitHub의 Jupyter 노트북에 다루어져 있습니다.

3. 확대
	아래는 100%에서 130% 사이로 무작위 확대하는 코드와 그 결과로 생성된 확대된 이미지(오른쪽)입니다.
	![[Pasted image 20241201170942.png]]


### 7. 이미지 전처리
---
이미지를 Nvidia 모델이 수용할 수 있는 색상 공간과 크기로 변경해야 한다. 먼저, Nvidia 연구 논문에서는 입력 이미지의 해상도를 200x66 픽셀로 요구한다. Part 4에서 했던 것처럼, 이미지의 윗부분 절반은 조향각 예측에 관련이 없기 때문에 잘라낼 것이다. 

두 번째로, 이미지는 YUV 색상 공간에 있어야 합니다. 우리는 간단히 `cv2.cvtColor()`를 사용하여 이를 처리할 것이다. 마지막으로 이미지를 정규화해야 한다.


``` python
def img_preprocess(image):
    height, _, _ = image.shape
    image = image[int(height/2):,:,:]  # remove top half of the image, as it is not relevant for lane following
    image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)  # Nvidia model said it is best to use YUV color space
    image = cv2.GaussianBlur(image, (3,3), 0)
    image = cv2.resize(image, (200,66)) # input image size (200,66) Nvidia model
    image = image / 255 # normalizing
    return image
```


### 8. **Nvidia Model**
---
![[Pasted image 20241201171115.png]]

``` python
# 파이썬 표준 라이브러리 임포트
import os
import random
import fnmatch
import datetime
import pickle

# 데이터 처리 라이브러리 임포트
import numpy as np
np.set_printoptions(formatter={'float_kind': lambda x: "%.4f" % x})

import pandas as pd
pd.set_option('display.width', 300)
pd.set_option('display.float_format', '{:,.4f}'.format)
pd.set_option('display.max_colwidth', 200)

# PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, random_split
print(f'torch.__version__: {torch.__version__}')

# 사이킷런 라이브러리 임포트
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

# 이미지 처리 관련 라이브러리 임포트
import cv2
from imgaug import augmenters as img_aug
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image

# 이미지 로드 및 데이터셋 준비
!cd /content
!git clone https://github.com/dctian/DeepPiCar

!ls

data_dir = '/content/DeepPiCar/models/lane_navigation/data/images'
file_list = os.listdir(data_dir)
image_paths = []
steering_angles = []
pattern = "*.png"
for filename in file_list:
    if fnmatch.fnmatch(filename, pattern):
        image_paths.append(os.path.join(data_dir, filename))
        angle = int(filename[-7:-4])  # 092 부분은 video01_143_092.png에서 각도를 나타냄. 90은 직진을 의미
        steering_angles.append(angle)

# 신경망 모델 정의
class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        # Conv2D 레이어 정의
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)
        
        # MaxPool 레이어 정의
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        
        # Dropout 레이어 정의
        self.dropout = nn.Dropout(0.5)
        
        # Fully connected 레이어 정의
        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # 입력 크기는 Conv 레이어와 Pooling의 결과에 따라 조정해야 함
        self.fc2 = nn.Linear(512, 10)  # 출력 클래스 수 (예: 10 클래스 분류 문제)

    def forward(self, x):
        # Convolution, Activation, and Pooling Layers
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        
        # Flatten
        x = x.view(-1, 128 * 4 * 4)  # Flattening the tensor for the fully connected layer
        
        # Fully Connected Layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Nvidia 모델 정의 (PyTorch로 변환)
class NvidiaModel(nn.Module):
    def __init__(self):
        super(NvidiaModel, self).__init__()
        # Convolutional Layers
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=24, kernel_size=5, stride=2)
        self.conv2 = nn.Conv2d(in_channels=24, out_channels=36, kernel_size=5, stride=2)
        self.conv3 = nn.Conv2d(in_channels=36, out_channels=48, kernel_size=5, stride=2)
        self.conv4 = nn.Conv2d(in_channels=48, out_channels=64, kernel_size=3)
        self.conv5 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)
        
        # Dropout Layer
        self.dropout = nn.Dropout(0.2)
        
        # Fully Connected Layers
        self.fc1 = nn.Linear(64 * 1 * 18, 100)  # Flatten 이후 입력 크기 (Conv 결과에 따라 조정 필요)
        self.fc2 = nn.Linear(100, 50)
        self.fc3 = nn.Linear(50, 10)
        self.fc4 = nn.Linear(10, 1)  # 출력 레이어: 조향각

    def forward(self, x):
        x = F.elu(self.conv1(x))
        x = F.elu(self.conv2(x))
        x = F.elu(self.conv3(x))
        x = F.elu(self.conv4(x))
        x = self.dropout(F.elu(self.conv5(x)))
        
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = F.elu(self.fc1(x))
        x = self.dropout(x)
        x = F.elu(self.fc2(x))
        x = F.elu(self.fc3(x))
        x = self.fc4(x)
        return x

# Nvidia 모델 초기화
nvidia_model = NvidiaModel()

# 손실 함수 및 옵티마이저 정의 (회귀 문제이므로 MSE 사용)
criterion = nn.MSELoss()
optimizer = optim.Adam(nvidia_model.parameters(), lr=1e-3)

# 모델 정보 출력
print(nvidia_model)

```

우리가 Nvidia 모델 아키텍처를 비교적 충실하게 구현했음을 유의하저. 단, 모델 외부에서 정규화를 수행하기 때문에 정규화 계층을 제거했고, 모델을 더 견고하게 만들기 위해 몇 가지 드롭아웃 계층을 추가했다. 

우리가 사용하는 손실 함수는 평균 제곱 오차(MSE)이며, 이는 회귀 학습을 하기 때문이다. 또한 익숙한 ReLU(정류 선형 유닛) 대신 ELU(지수 선형 유닛) 활성화 함수를 사용했는데, ELU는 x가 음수일 때 발생하는 "죽은 ReLU" 문제를 피할 수 있기 때문이다.


![[Pasted image 20241201171531.png]]


### 8. 도로 주행
---
모델이 정말로 좋은지는 결국 실제 주행에서 테스트되어야 한다. 아래는 PiCar를 운전하는 핵심 로직이다. 우리가 직접 코딩했던 차선 내비게이션 구현과 비교해 보면, 파란색을 감지하고, 차선을 감지하고, 조향각을 계산하는 등 모든 단계(약 200줄의 코드)가 사라졌다. 

대신에 단순한 명령어들인 `load_model`과 `model.predict()`가 이를 대체한다. 물론, 이는 모든 보정이 학습 단계에서 이루어졌고, 3MB 크기의 HDFS 포맷으로 저장된 학습된 모델 파일이 무려 250,000개의 파라미터를 포함하고 있기 때문이다.

``` python
model = load_model('lane_navigation_model.h5')
        
def compute_steering_angle(self, frame):
    preprocessed = img_preprocess(frame)
    X = np.asarray([preprocessed])
    steering_angle = model.predict(X)[0]
    return steering_angle
```