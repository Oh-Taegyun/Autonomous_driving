>https://towardsdatascience.com/deeppicar-part-6-963334b2abe0

진정한 자율 주행 차량은 항상 주변을 인식해야 한다. 이번 글에서는 또 다른 중요한 인식 기능인 교통 표지판 및 보행자 감지에 대해 이야기할 것이다. 이 기능은 2019년의 차량에서는 사용할 수 없으며, 아마도 테슬라만 가능할 것이다. 

우리는 DeepPiCar가 (축소된) 교통 표지판과 보행자를 실시간으로 식별하고 반응할 수 있도록 학습시킬 것이다.

### 1. 인식: 교통 표지판 및 보행자 감지  
---
객체 감지는 컴퓨터 비전과 딥러닝에서 잘 알려진 문제이다. 객체 감지 모델에는 기본 신경망과 감지 신경망이라는 두 가지 구성 요소가 있다.

먼저, 기본 신경망은 이미지에서 선, 에지, 원과 같은 저수준 특징부터 얼굴, 사람, 신호등, 정지 신호와 같은 고수준 특징까지 특징을 추출하는 CNN이다. 잘 알려진 기본 신경망으로는 LeNet, InceptionNet(GoogleNet이라고도 함), ResNet, VGG-Net, AlexNet, MobileNet 등이 있다. 이 훌륭한 글은 아래에서 이러한 기본 신경망 간의 차이점을 설명한다.

감지 신경망은 기본 신경망의 끝에 연결되어 추출된 특징을 바탕으로 단일 이미지에서 여러 객체를 동시에 식별하는 데 사용된다. 인기 있는 감지 신경망으로는 SSD(Single Shot MultiBox Detector), R-CNN(Region with CNN features), Faster R-CNN, YOLO(You Only Look Once) 등이 있다. 이 훌륭한 글은 이러한 감지 신경망 간의 차이점을 설명한다.


![[Pasted image 20241201172226.png]]

객체 감지 모델은 일반적으로 기본 신경망 유형과 감지 신경망 유형의 조합으로 이름이 붙는다. 예를 들어, "MobileNet SSD" 모델, "Inception SSD" 모델, 또는 "ResNet Faster R-CNN" 모델 등이 그 예이다.

마지막으로, 사전 학습된 감지 모델의 경우 모델 이름에는 학습된 이미지 데이터셋의 유형도 포함된다. 이미지 분류기 및 감지기 학습에 사용된 몇 가지 잘 알려진 데이터셋으로는 COCO 데이터셋(약 100개의 일반적인 가정용 물체), Open Images 데이터셋(약 20,000가지 종류의 객체), iNaturalist 데이터셋(약 200,000가지의 동식물 종)이 있다. 

예를 들어, `ssd_mobilenet_v2_coco` 모델은 MobileNet의 2번째 버전을 사용하여 특징을 추출하고, SSD를 사용해 객체를 감지하며, COCO 데이터셋으로 사전 학습되었다.

이 모든 모델의 조합을 추적하는 것은 쉽지 않은 일이다. 하지만 구글 덕분에, 그들은 TensorFlow로 사전 학습된 모델 목록(일명 Model Zoo, 실제로 모델의 동물원)을 공개하여, 필요한 모델을 다운로드하여 프로젝트에서 객체 감지 추론에 직접 사용할 수 있게 했다.


### 2. 전이 학습
---
하지만 이 글은 책이나 소파가 아닌, 우리의 미니 교통 표지판과 보행자를 감지하는 것이 목적이다. 그렇다고 수십만 개의 이미지를 수집하고 라벨링하며 몇 주 또는 몇 달 동안 깊은 감지 모델을 처음부터 구축하고 학습하고 싶지는 않다. 

우리가 할 수 있는 것은 전이 학습(Transfer Learning)을 활용하는 것이다. 이는 사전 학습된 모델의 파라미터를 시작점으로 삼아, 우리 자신의 이미지와 라벨 50-100개만 제공하고, 감지 신경망의 일부만 몇 시간 동안 학습하는 방법이다. 직관적으로 사전 학습된 모델의 기본 CNN 계층은 대량의 다양한 이미지로 학습되었기 때문에 이미 이미지로부터 특징을 잘 추출할 수 있다. 차이점은 우리가 이제 사전 학습된 모델의 객체 유형(~100-100,000가지)과는 다른 객체 유형(6가지)을 가지고 있다는 것이다.

우리는 총 3가지 객체 유형을 만들어야 한다. 즉, 빨간불, 초록불, 장애물이다. 

![[Pasted image 20241201172734.png]]

이미지에는 지금 다양한 표지판도 있지만 우리 프로젝트에는 해당되지 않는 내용이다. 

그래서 비슷한 사진을 약 50장 찍어야 하며, 각 이미지에 객체를 무작위로 배치해야 한다.

그 다음 각 이미지에서 각 객체에 대해 경계 상자를 사용해 라벨링했다. 시간이 오래 걸릴 것처럼 보였지만, `labelImg`라는 무료 도구(Windows/Mac/Linux용)를 사용하여 이 어려운 작업을 매우 쉽게 할 수 있다.

제가 해야 했던 것은 학습 이미지를 저장한 폴더를 `labelImg`에 지정하고, 각 이미지에서 객체 주위에 상자를 그린 후 객체 유형을 선택하는 것이다(새로운 유형이라면 빠르게 새로 만들 수 있었다). 

이후에는 이미지를 라벨 XML 파일과 함께 학습 및 테스트 폴더로 무작위로 나누었다. 제 학습/테스트 데이터셋은 DeepPiCar의 GitHub 저장소의 `models/object_detection/data` 폴더에서 찾을 수 있다.

### 3. 모델 선택
---
Raspberry Pi에서는 제한된 계산 능력 때문에 속도와 정확도를 모두 고려하여 상대적으로 빠르게 실행되는 모델을 선택해야 한다. 몇 가지 모델을 실험한 후, 속도와 정확성 간의 최적 균형으로 MobileNet v2 SSD COCO 모델을 선택했다. 


### 4. 전이 학습/모델 학습/테스트
---
이 단계에서는 다시 Google Colab을 사용할 것이다. 

전체 노트북과 출력이 꽤 길기 때문에, 아래에서 제 Jupyter Notebook의 주요 부분을 제시하겠다. 전체 노트북 코드는 GitHub에서 찾을 수 있으며, 각 단계에 대해 매우 상세한 설명이 포함되어 있다.

``` python
# 사용하려는 레포지토리 URL을 정의합니다.
# 레포지토리를 포크(fork)한 경우, 해당 URL을 대체할 수 있습니다.
repo_url = 'https://github.com/dctian/DeepPiCar'

# 훈련 단계의 수를 설정합니다.
num_steps = 1000  # 총 200,000번 훈련하는 것이 원래 목표이나, 현재는 1000번으로 설정

# 평가 단계의 수를 설정합니다.
num_eval_steps = 50  # 평가 시 50 스텝을 실행

# 모델 구성 정보 (모델 Zoo의 GitHub에서 참조)
# 여기에는 각 모델의 이름, 파이프라인 설정 파일, 배치 크기 정보가 포함되어 있습니다.
MODELS_CONFIG = {
    # SSD Mobilenet V1 모델 구성
    # URL에서 모델을 다운로드할 수 있습니다.
    'ssd_mobilenet_v1_quantized': {
        'model_name': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18',  # 모델 이름
        'pipeline_file': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync.config',  # 모델의 파이프라인 설정 파일
        'batch_size': 12  # 훈련 시 사용하는 배치 크기
    },
    # SSD Mobilenet V2 모델 구성
    # URL에서 모델을 다운로드할 수 있습니다.
    'ssd_mobilenet_v2_quantized': {
        'model_name': 'ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03',  # 모델 이름
        'pipeline_file': 'ssd_mobilenet_v2_quantized_300x300_coco.config',  # 모델의 파이프라인 설정 파일
        'batch_size': 12  # 훈련 시 사용하는 배치 크기
    },
}

# 사용할 모델을 선택합니다.
selected_model = 'ssd_mobilenet_v2_quantized'  # ssd_mobilenet_v2_quantized 모델을 사용하도록 선택

# 선택한 모델의 이름을 설정합니다.
MODEL = MODELS_CONFIG[selected_model]['model_name']

# 텐서플로우 오브젝트 디텍션 API에서 사용하는 파이프라인 설정 파일 이름을 설정합니다.
pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']

# 훈련 배치 크기를 설정합니다. 이 값은 Colab의 Tesla K80 GPU 메모리 용량에 맞추어 설정되었습니다.
batch_size = MODELS_CONFIG[selected_model]['batch_size']

# 레포지토리 DeepPiCar를 복제합니다.
!git clone https://github.com/dctian/DeepPiCar

# 현재 디렉터리를 /content로 이동합니다.
%cd /content

# 텐서플로우 모델 레포지토리를 복제합니다. 이를 통해 다양한 텐서플로우 모델에 접근할 수 있습니다.
!git clone --quiet https://github.com/tensorflow/models.git

# 필요한 패키지들을 설치합니다.
# protobuf-compiler, PIL(Python Imaging Library), lxml, tk 등을 설치합니다.
!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk

# 파이썬 패키지를 설치합니다.
# Cython, contextlib2, pillow, lxml, matplotlib 등 여러 가지 데이터 처리 및 시각화 패키지를 설치합니다.
!pip install -q Cython contextlib2 pillow lxml matplotlib

# COCO 데이터셋 관련 툴인 pycocotools를 설치합니다.
!pip install -q pycocotools

# 모델 디렉터리로 이동합니다.
%cd /content/models/research

# TensorFlow 오브젝트 디텍션 API에서 사용하는 .proto 파일들을 컴파일하여 Python 파일로 변환합니다.
!protoc object_detection/protos/*.proto --python_out=.

# 환경 변수를 설정하여 PYTHONPATH에 TensorFlow 모델 경로를 추가합니다.
# 이를 통해 모델 관련 파일들을 임포트할 수 있습니다.
import os
os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'

```


### 5. 계획 및 운동 제어
---
이제 DeepPiCar가 앞에 있는 객체를 감지하고 식별할 수 있게 되었으므로, 해당 객체에 대해 무엇을 해야 하는지 알려줘야 한다. 즉, 운동 제어이다. 

운동 제어에는 두 가지 접근 방식이 있다. 
1. 규칙 기반(rule-based) 접근 방식
	- 규칙 기반 접근 방식은 차량이 각 객체를 만났을 때 정확히 무엇을 해야 할지 알려주는 것이다. 예를 들어, 빨간불이나 보행자를 보면 차량에 멈추라고 하거나, 낮은 속도 제한 표지판을 보면 더 느리게 주행하라고 하는 것 등이 이에 해당힌다. 
2. 종단 간(end-to-end) 접근 방식
	- 종단 간 접근 방식은 간단히 말해 좋은 운전자들의 비디오 영상을 많이 제공하고, 차량이 딥러닝을 통해 스스로 빨간불 앞에서 멈추고 보행자 앞에서 서며, 속도 제한이 낮아질 때 속도를 줄이는 것을 학습하는 것이다. 

이번 글에서는 규칙 기반 접근 방식을 선택했다. 그 이유는 
1) 우리가 도로 규칙을 배우는 방식으로 운전을 배우기 때문이며, 
2) 구현이 더 간단하기 때문이다.

우리는 총 3가지 객체 유형(빨간불, 초록불, 보행자)이 있으므로, 몇 가지 객체 유형을 다루는 방법을 설명하고, 전체 구현은 GitHub의 두 파일 `traffic_objects.py`와 `object_on_road_processor.py`에서 확인하실 수 있다.

규칙은 매우 간단합니다. 객체가 감지되지 않으면 마지막으로 알려진 속도 제한으로 주행한다. 어떤 객체가 감지되면 해당 객체가 차량의 속도나 속도 제한을 수정한다. 예를 들어, 충분히 가까운 빨간불이 감지되면 정지하고, 빨간불이 감지되지 않으면 출발한다.

먼저, `TrafficObject`라는 기본 클래스를 정의할 것입니다. 이 클래스는 도로에서 감지할 수 있는 모든 교통 표지판이나 보행자를 나타낸다. 이 클래스에는 `set_car_state(car_state)`라는 메서드가 포함되어 있다. `car_state` 딕셔너리에는 속도와 속도 제한 두 변수가 포함되어 있으며, 이 메서드에 의해 변경된다. 

또한 객체가 충분히 가까운지 확인하는 헬퍼 메서드 `is_close_by()`도 포함되어 있다. (참고로, 단일 카메라로는 거리를 정확히 측정할 수 없기 때문에, 객체의 높이로 거리를 근사하고 있다. 거리를 정확하게 측정하려면 라이다(Lidar)나 그와 유사한 초음파 센서, 혹은 Tesla와 같은 스테레오 비전 카메라 시스템이 필요하다.)


``` python
# 교통 객체를 나타내는 기본 클래스
class TrafficObject(object):

    # 자동차 상태를 설정하는 메서드 (기본적으로 아무 동작도 하지 않음)
    def set_car_state(self, car_state):
        pass

    # 객체가 가까이 있는지 여부를 판단하는 정적 메서드
    @staticmethod
    def is_close_by(obj, frame_height, min_height_pct=0.05):
        # 객체가 프레임 높이의 일정 비율 이상을 차지하는지 확인하여 가까이 있는지 판단
        # 기본값: 객체가 프레임 높이의 5% 이상을 차지하면 가까이 있다고 판단
        obj_height = obj.bounding_box[1][1] - obj.bounding_box[0][1]
        return obj_height / frame_height > min_height_pct

```

빨간불과 보행자에 대한 구현은 매우 간단하며, 단순히 차량 속도를 0으로 설정합니다.

``` python
# 빨간 신호등 객체를 나타내는 클래스 (TrafficObject를 상속)
class RedTrafficLight(TrafficObject):
    # 자동차 상태를 설정하는 메서드
    def set_car_state(self, car_state):
        logging.debug('red light: stopping car')  # 빨간불이 켜졌을 때 자동차를 멈춤
        car_state['speed'] = 0  # 자동차의 속도를 0으로 설정

# 보행자 객체를 나타내는 클래스 (TrafficObject를 상속)
class Pedestrian(TrafficObject):
    # 자동차 상태를 설정하는 메서드
    def set_car_state(self, car_state):
        logging.debug('pedestrian: stopping car')  # 보행자가 있을 때 자동차를 멈춤
        car_state['speed'] = 0  # 자동차의 속도를 0으로 설정
```

25mph와 40mph 속도 제한 표지판 모두 동일한 `SpeedLimit` 클래스를 사용할 수 있으며, `speed_limit`을 초기화 매개변수로 받습니다. 표지판이 감지되면 차량의 속도 제한을 해당 제한 속도로 설정하면 됩니다.

``` python
# 속도 제한 표지 객체를 나타내는 클래스 (TrafficObject를 상속)
class SpeedLimit(TrafficObject):

    # 생성자: 속도 제한 값을 설정
    def __init__(self, speed_limit):
        self.speed_limit = speed_limit

    # 자동차 상태를 설정하는 메서드
    def set_car_state(self, car_state):
        logging.debug('speed limit: set limit to %d' % self.speed_limit)  # 속도 제한 값을 로그에 기록
        car_state['speed_limit'] = self.speed_limit  # 자동차의 속도 제한 값을 설정
```

초록불에 대한 구현은 더 간단하며, 초록불이 감지되었음을 출력하는 것 외에는 아무것도 하지 않는다(코드는 생략). 정지 표지판에 대한 구현은 조금 더 복잡한데, 상태를 추적해야 하기 때문이다. 

즉, 차량이 정지 표지판에서 이미 몇 초 동안 멈춰 있었다는 것을 기억해야 하며, 차량이 표지판을 지나치면서 이후 비디오 이미지에 매우 큰 정지 표지판이 포함되더라도 앞으로 나아가야 한다. 자세한 내용은 `traffic_objects.py`에서 제 구현을 참조.

각 교통 표지판에 대한 동작을 정의한 후에는 이를 모두 연결할 클래스를 정의해야 힌다. 그것이 `ObjectsOnRoadProcessor` 클래스이다. 이 클래스는 먼저 Edge TPU를 위한 학습된 모델을 로드하고, 모델을 사용해 실시간 비디오에서 객체를 감지한 후, 각 교통 객체를 호출하여 차량의 속도와 속도 제한을 변경한다. 아래는 `objects_on_road_processor.py`의 주요 부분입니다.

각 `TrafficObject`는 `car_state` 객체의 속도와 속도 제한만 변경하며, 실제 차량의 속도를 변경하지는 않다. 모든 교통 표지판과 보행자를 감지하고 처리한 후 실제 차량의 속도를 변경하는 것은 `ObjectsOnRoadProcessor`이다.

### 6. 전체 조합
---
``` python
class ObjectsOnRoadProcessor(object):
    """
    이 클래스는 1) 도로 위의 객체(교통 신호 및 사람)를 감지하고 
    2) 그에 따라 자동차의 네비게이션(속도/조향)을 제어합니다.
    """

    def __init__(self,
                 car=None,
                 speed_limit=40,
                 model='/home/pi/DeepPiCar/models/object_detection/data/model_result/road_signs_quantized_edgetpu.tflite',
                 label='/home/pi/DeepPiCar/models/object_detection/data/model_result/road_sign_labels.txt',
                 width=640,
                 height=480):
        # 모델: 반드시 Edge TPU용으로 컴파일된 tflite 모델이어야 합니다.
        # https://coral.withgoogle.com/web-compiler/
        logging.info('Creating a ObjectsOnRoadProcessor...')
        self.width = width  # 프레임의 너비 설정
        self.height = height  # 프레임의 높이 설정

        # 자동차 초기화
        self.car = car  # 제어할 자동차 객체
        self.speed_limit = speed_limit  # 초기 속도 제한 설정
        self.speed = speed_limit  # 현재 속도 초기화

        # TensorFlow 모델 초기화 (라벨 파일 로드)
        with open(label, 'r') as f:
            pairs = (l.strip().split(maxsplit=1) for l in f.readlines())  # 라벨 파일에서 라벨을 읽어옴
            self.labels = dict((int(k), v) for k, v in pairs)  # 라벨 ID와 이름을 딕셔너리로 저장

        # Edge TPU 엔진 초기화
        logging.info('Initialize Edge TPU with model %s...' % model)
        self.engine = edgetpu.detection.engine.DetectionEngine(model)  # Edge TPU 엔진 생성
        self.min_confidence = 0.30  # 감지 객체의 최소 신뢰도 설정
        self.num_of_objects = 3  # 최대 감지 객체 수 설정
        logging.info('Initialize Edge TPU with model done.')

        # 도로 위 객체 초기화
        self.traffic_objects = {
            0: GreenTrafficLight(),  # 녹색 신호등
            1: Person(),  # 보행자
            2: RedTrafficLight(),  # 빨간 신호등
            3: SpeedLimit(25),  # 속도 제한 25
            4: SpeedLimit(40),  # 속도 제한 40
            5: StopSign()  # 정지 표지판
        }

    # 도로 위의 객체를 처리하는 메서드
    def process_objects_on_road(self, frame):
        # 도로 객체 처리의 메인 진입점
        objects, final_frame = self.detect_objects(frame)  # 객체 감지
        self.control_car(objects)  # 자동차 제어
        return final_frame

    # 자동차를 제어하는 메서드
    def control_car(self, objects):
        logging.debug('Control car...')
        car_state = {"speed": self.speed_limit, "speed_limit": self.speed_limit}  # 초기 자동차 상태 설정

        # 객체가 감지되지 않았을 경우
        if len(objects) == 0:
            logging.debug('No objects detected, drive at speed limit of %s.' % self.speed_limit)

        contain_stop_sign = False  # 정지 표지판이 있는지 여부

        # 감지된 각 객체에 대해 처리
        for obj in objects:
            obj_label = self.labels[obj.label_id]  # 객체의 라벨 이름 가져오기
            processor = self.traffic_objects[obj.label_id]  # 해당 객체에 대한 처리기 가져오기

            # 객체가 가까이에 있는 경우에만 처리
            if processor.is_close_by(obj, self.height):
                processor.set_car_state(car_state)  # 객체에 따라 자동차 상태 설정
            else:
                logging.debug("[%s] object detected, but it is too far, ignoring. " % obj_label)  # 너무 먼 객체는 무시
            if obj_label == 'Stop':
                contain_stop_sign = True  # 정지 표지판이 있으면 표시

        # 정지 표지판이 없는 경우 정지 상태 해제
        if not contain_stop_sign:
            self.traffic_objects[5].clear()

        # 자동차 상태에 따라 주행 재개
        self.resume_driving(car_state)

    # 자동차 주행 재개 메서드
    def resume_driving(self, car_state):
        old_speed = self.speed  # 이전 속도 저장
        self.speed_limit = car_state['speed_limit']  # 새로운 속도 제한 설정
        self.speed = car_state['speed']  # 새로운 속도 설정

        # 자동차가 정지 상태이면 속도 0으로 설정
        if self.speed == 0:
            self.set_speed(0)
        else:
            self.set_speed(self.speed_limit)  # 그렇지 않으면 속도 제한으로 설정
        logging.debug('Current Speed = %d, New Speed = %d' % (old_speed, self.speed))

        # 자동차가 정지 상태일 때 1초 동안 멈춤
        if self.speed == 0:
            logging.debug('full stop for 1 seconds')
            time.sleep(1)

    # 자동차의 속도를 설정하는 메서드
    def set_speed(self, speed):
        # 자동차가 없는 경우 테스트를 위해 speed만 설정 가능하도록 함
        self.speed = speed
        if self.car is not None:
            logging.debug("Actually setting car speed to %d" % speed)
            self.car.back_wheels.speed = speed  # 실제 자동차 속도 설정

```
